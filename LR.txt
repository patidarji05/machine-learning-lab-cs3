{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "LR.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJM8ymxD96uR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " class MyLinearRegression:\n",
        "    def __init__(self, weight= 10 , bias=  7  , learning_rate=   0.006,\n",
        "                 iterations=   10 ):\n",
        "        self.weight = weight\n",
        "        self.bias = bias\n",
        "        self.learning_rate = learning_rate\n",
        "        self.iterations = iterations\n",
        "        self.cost_trend = []\n",
        "        self.cost = 0\n",
        "\n",
        "    def predict(self, x):\n",
        "        predicted_set = []\n",
        "        for i in range(len(x)):\n",
        "            predicted_value = self.weight * x[i] + self.bias\n",
        "            predicted_set.append(predicted_value)\n",
        "        return predicted_set\n",
        "\n",
        "    def cost_function(self, x, y):\n",
        "        count = len(x)\n",
        "        total_error = 0.0\n",
        "        for i in range(count):\n",
        "            total_error += (y[i] - (self.weight * x[i] +\n",
        "                            self.bias)) ** 2\n",
        "        return float(total_error) / (2 * count)\n",
        "\n",
        "    def update_weights(self, x, y):\n",
        "        weight_deriv = 0\n",
        "        bias_deriv = 0\n",
        "        count = len(x)\n",
        "\n",
        "        for i in range(count):\n",
        "            # Calculate partial derivatives\n",
        "            # -2x(y - (mx + b))\n",
        "            weight_deriv += -2 * x[i] * (y[i] -(self.weight * x[i] + self.bias))\n",
        "\n",
        "            # -2(y - (mx + b))\n",
        "            bias_deriv += -2 * (y[i] - (self.weight * x[i] +\n",
        "                                self.bias))\n",
        "\n",
        "        # We subtract because the derivatives point in direction of steepest\n",
        "        # ascent\n",
        "        self.weight -= (weight_deriv / count) * self.learning_rate\n",
        "        self.bias -= (bias_deriv / count) * self.learning_rate\n",
        "\n",
        "    def train(self, x, y):\n",
        "        for i in range(self.iterations):\n",
        "            self.update_weights(x, y)\n",
        "            # Calculating cost\n",
        "            self.cost = self.cost_function(x, y)\n",
        "            self.cost_trend.append(self.cost)\n",
        "           # if i % 10000 == 0:\n",
        "            print(\"Iteration: {}\\t Weight: {}\\t Bias: {}\\t Cost: {}\".format(i, self.weight, self.bias, self.cost))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGMUmPfj96ua",
        "colab_type": "code",
        "outputId": "52e5c6bb-7f51-4386-f5c1-680760c7a6df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 824
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# intialise data of lists. \n",
        "data = {'Hours':[2.5,5.1,3.2,8.5,3.5,1.5,9.2,5.5,8.3,2.7,7.7,5.9,4.5,3.3,1.1,8.9,2.5,1.9,6.1,7.4,2.7,4.8,3.8,6.9,7.8], \n",
        "        'Scores':[21,47,27,75,30,20,88,60,81,25,85,62,41,42,17,95,30,24,67,69,30,54,35,76,86]} \n",
        "  \n",
        "# Create DataFrame \n",
        "studentscores = pd.DataFrame(data) \n",
        "  \n",
        "# Print the output. \n",
        "studentscores "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Hours</th>\n",
              "      <th>Scores</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.5</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5.1</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.2</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8.5</td>\n",
              "      <td>75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3.5</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.5</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>9.2</td>\n",
              "      <td>88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>5.5</td>\n",
              "      <td>60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8.3</td>\n",
              "      <td>81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2.7</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>7.7</td>\n",
              "      <td>85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>5.9</td>\n",
              "      <td>62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>4.5</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>3.3</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1.1</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>8.9</td>\n",
              "      <td>95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2.5</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1.9</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>6.1</td>\n",
              "      <td>67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>7.4</td>\n",
              "      <td>69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>2.7</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>4.8</td>\n",
              "      <td>54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>3.8</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>6.9</td>\n",
              "      <td>76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>7.8</td>\n",
              "      <td>86</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Hours  Scores\n",
              "0     2.5      21\n",
              "1     5.1      47\n",
              "2     3.2      27\n",
              "3     8.5      75\n",
              "4     3.5      30\n",
              "5     1.5      20\n",
              "6     9.2      88\n",
              "7     5.5      60\n",
              "8     8.3      81\n",
              "9     2.7      25\n",
              "10    7.7      85\n",
              "11    5.9      62\n",
              "12    4.5      41\n",
              "13    3.3      42\n",
              "14    1.1      17\n",
              "15    8.9      95\n",
              "16    2.5      30\n",
              "17    1.9      24\n",
              "18    6.1      67\n",
              "19    7.4      69\n",
              "20    2.7      30\n",
              "21    4.8      54\n",
              "22    3.8      35\n",
              "23    6.9      76\n",
              "24    7.8      86"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Avh401TI11sH",
        "colab_type": "code",
        "outputId": "f41afd1c-6ae1-441b-a977-4dff9f4eeb25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "x=[2.5,5.1,3.2,8.5,3.5,1.5,9.2,5.5,8.3,2.7,7.7,5.9,4.5,3.3,1.1,8.9,2.5,1.9,6.1,7.4,2.7,4.8,3.8,6.9,7.8] \n",
        "y=[21,47,27,75,30,20,88,60,81,25,85,62,41,42,17,95,30,24,67,69,30,54,35,76,86]\n",
        "plt.scatter(x,y,s=10)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEGCAYAAABiq/5QAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nesbgl6SeMfglqWf+PyxzPdxnrKOcAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LSp2jKt96uj",
        "colab_type": "code",
        "outputId": "a1399a00-b891-47f0-805f-955cce4bc515",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "#from my_linear_regression import MyLinearRegression\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Importing the dataset\n",
        "\n",
        "X = studentscores.iloc[:, :-1].values\n",
        "y = studentscores.iloc[:, -1].values\n",
        "X,y"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[2.5],\n",
              "        [5.1],\n",
              "        [3.2],\n",
              "        [8.5],\n",
              "        [3.5],\n",
              "        [1.5],\n",
              "        [9.2],\n",
              "        [5.5],\n",
              "        [8.3],\n",
              "        [2.7],\n",
              "        [7.7],\n",
              "        [5.9],\n",
              "        [4.5],\n",
              "        [3.3],\n",
              "        [1.1],\n",
              "        [8.9],\n",
              "        [2.5],\n",
              "        [1.9],\n",
              "        [6.1],\n",
              "        [7.4],\n",
              "        [2.7],\n",
              "        [4.8],\n",
              "        [3.8],\n",
              "        [6.9],\n",
              "        [7.8]]),\n",
              " array([21, 47, 27, 75, 30, 20, 88, 60, 81, 25, 85, 62, 41, 42, 17, 95, 30,\n",
              "        24, 67, 69, 30, 54, 35, 76, 86]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvfKE_WT96un",
        "colab_type": "code",
        "outputId": "de925551-9d84-4dea-fb6f-aa6b335f3c06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/3, random_state=0)\n",
        "\n",
        "# Fitting Simple Linear Regression to the Training set\n",
        "regressor = MyLinearRegression()\n",
        "regressor.train(X_train, y_train)\n",
        "print('Weight: ' + str(regressor.weight) + ' Bias: ' + str(regressor.bias))\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = regressor.predict(X_test)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 0\t Weight: [9.58885]\t Bias: [6.9265]\t Cost: 23.135375676748456\n",
            "Iteration: 1\t Weight: [9.35078631]\t Bias: [6.8799078]\t Cost: 19.258603470158377\n",
            "Iteration: 2\t Weight: [9.21319828]\t Bias: [6.84894413]\t Cost: 17.947252059599595\n",
            "Iteration: 3\t Weight: [9.13393524]\t Bias: [6.82706135]\t Cost: 17.500424432254643\n",
            "Iteration: 4\t Weight: [9.08852868]\t Bias: [6.81045851]\t Cost: 17.34494766628923\n",
            "Iteration: 5\t Weight: [9.06277455]\t Bias: [6.79692914]\t Cost: 17.28767232365746\n",
            "Iteration: 6\t Weight: [9.0484274]\t Bias: [6.78519236]\t Cost: 17.26351029770071\n",
            "Iteration: 7\t Weight: [9.04070071]\t Bias: [6.7745046]\t Cost: 17.250527932462344\n",
            "Iteration: 8\t Weight: [9.03681591]\t Bias: [6.76443419]\t Cost: 17.24133387471509\n",
            "Iteration: 9\t Weight: [9.03516004]\t Bias: [6.75473053]\t Cost: 17.233437275733337\n",
            "Weight: [9.03516004] Bias: [6.75473053]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qw0Vfxf996uw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}